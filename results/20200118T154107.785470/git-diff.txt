diff --git a/NSMGame.py b/NSMGame.py
index be5d777..07aaa58 100644
--- a/NSMGame.py
+++ b/NSMGame.py
@@ -74,8 +74,8 @@ class ObservationSpace(object):
         :param link_num:
         :param req_time:
         """
-        self.observation_size = args.window * (node_num + 2 * link_num) + req_time
-        self.observation_space = np.zeros(shape=(self.observation_size, ), dtype=np.float32)
+        self.observation_size = 10 * (args.window * (node_num + 2 * link_num) + req_time)
+        self.observation_space = np.zeros(shape=(self.observation_size,), dtype=np.float32)
 
 
 class Game(object):
@@ -117,7 +117,6 @@ class Game(object):
         self.action_space = ActionSpace(10)
         self.success_request = 0
 
-
     def reset(self):
 
         """
@@ -151,16 +150,8 @@ class Game(object):
         # return the first state
         self.time = self.request[0].arrival_time
         observation = self.get_state_link(self.time)
-        reward = INIT
-        done = False
-        info = None
 
-        return observation, reward, done, info
-
-    def get_state_link(self, time):
-        tim = time
-        state = np.zeros(shape=(370, ), dtype=np.float32)
-        return state
+        return observation
 
     def get_state_link1(self, time):
 
@@ -229,7 +220,7 @@ class Game(object):
             return np.array([None, None]), 0, True, None
         #  --------------------------------------------------------------
         done = False
-        info = False
+        info = {}
         reward = 0
         # check if there are events (arrival of departure)
         if self.event[self.event_iter][0] > self.time:
@@ -255,7 +246,6 @@ class Game(object):
                         pass
                     self.event_iter += 1
                 else:
-                    info = True
                     req = self.request[self.event[self.event_iter][1]]
                     reward = self.exec_action(action, req)
                     logger.info('successfully stepped')
@@ -362,7 +352,6 @@ if __name__ == "__main__":
     obs, _, _, _ = game.reset()
     print(obs)
 
-
     # path = [4, 1, 2, 5]
     # game.network.set_wave_state(time_index=5, holding_time=3, wave_index=1, nodes=path, state=False, check=True)
     # print(game.network.is_allocable(path=path, wave_index=1, start_time=5, end_time=7))
@@ -376,7 +365,7 @@ if __name__ == "__main__":
     #     obs, reward, done, info = game.step(action)
     #     if done:
     #         print(done)
-        # print('obs? ', obs, 'reward? ', reward, 'done? ', done, 'info? ', info)
+    # print('obs? ', obs, 'reward? ', reward, 'done? ', done, 'info? ', info)
     # game.network.show_link_state()
     # print(done)
     # request = Request(1, 1, 6, 1, 4, 1)
diff --git a/Test/RL_pytorch.py b/Test/RL_pytorch.py
index 9c576c7..14bb070 100644
--- a/Test/RL_pytorch.py
+++ b/Test/RL_pytorch.py
@@ -6,6 +6,7 @@ import matplotlib
 import matplotlib.pyplot as plt
 from collections import namedtuple
 from itertools import count
+from PIL import Image
 
 import torch
 import torch.nn as nn
@@ -45,4 +46,198 @@ class ReplayMemory(object):
     def __len__(self):
         return len(self.memory)
 
-class 
\ No newline at end of file
+
+class DQN(nn.Module):
+
+    def __init__(self, h, w, outputs):
+        super(DQN, self).__init__()
+        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
+        self.bn1 = nn.BatchNorm2d(16)
+        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
+        self.bn2 = nn.BatchNorm2d(32)
+        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
+        self.bn3 = nn.BatchNorm2d(32)
+
+        def conv2d_size_out(size, kernel_size=5, stride=2):
+            return (size - (kernel_size - 1) - 1) // stride + 1
+
+        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))
+        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))
+        linear_input_size = convw * convh * 32
+        self.head = nn.Linear(linear_input_size, outputs)
+
+    def forward(self, x):
+        x = F.relu(self.bn1(self.conv1(x)))
+        x = F.relu(self.bn2(self.conv2(x)))
+        x = F.relu(self.bn3(self.conv3(x)))
+        return self.head(x.view(x.size(0), -1))
+
+
+resize = T.Compose([T.ToPILImage(),
+                    T.Resize(40, interpolation=Image.CUBIC),
+                    T.ToTensor()])
+
+
+def get_cart_location(screen_width):
+    world_width = env.x_threshold * 2
+    scale = screen_width / world_width
+    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART
+
+
+def get_screen():
+    # Returned screen requested by gym is 400x600x3, but is sometimes larger
+    # such as 800x1200x3. Transpose it into torch order (CHW).
+    screen = env.render(mode='rgb_array').transpose((2, 0, 1))
+    # Cart is in the lower half, so strip off the top and bottom of the screen
+    _, screen_height, screen_width = screen.shape
+    screen = screen[:, int(screen_height * 0.4):int(screen_height * 0.8)]
+    view_width = int(screen_width * 0.6)
+    cart_location = get_cart_location(screen_width)
+    if cart_location < view_width // 2:
+        slice_range = slice(view_width)
+    elif cart_location > (screen_width - view_width // 2):
+        slice_range = slice(-view_width, None)
+    else:
+        slice_range = slice(cart_location - view_width // 2,
+                            cart_location + view_width // 2)
+    # Strip off the edges, so that we have a square image centered on a cart
+    screen = screen[:, :, slice_range]
+    # Convert to float, rescale, convert to torch tensor
+    # (this doesn't require a copy)
+    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
+    screen = torch.from_numpy(screen)
+    # Resize, and add a batch dimension (BCHW)
+    return resize(screen).unsqueeze(0).to(device)
+
+
+env.reset()
+plt.figure()
+plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),
+           interpolation='none')
+plt.title('Example extracted screen')
+plt.show()
+
+BATCH_SIZE = 128
+GAMMA = 0.999
+EPS_START = 0.9
+EPS_END = 0.05
+EPS_DECAY = 200
+TARGET_UPDATE = 10
+
+init_screen = get_screen()
+_, _, screen_height, screen_width = init_screen.shape
+
+n_actions = env.action_space.n
+
+policy_net = DQN(screen_height, screen_width, n_actions).to(device)
+target_net = DQN(screen_height, screen_width, n_actions).to(device)
+target_net.load_state_dict(policy_net.state_dict())
+target_net.eval()
+
+optimizer = optim.RMSprop(policy_net.parameters())
+memory = ReplayMemory(10000)
+
+steps_done = 0
+
+
+def select_action(state):
+    global steps_done
+    sample = random.random()
+    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
+    steps_done += 1
+    if sample > eps_threshold:
+        return random.randrange(2)
+        # with torch.no_grad():
+            # return policy_net(state).max(1)[1].view(1, 1)
+    else:
+        return random.randrange(2)
+        #return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)
+
+
+episode_durations = []
+
+
+def plot_durations():
+    plt.figure(2)
+    plt.clf()
+    durations_t = torch.tensor(episode_durations, dtype=torch.float)
+    plt.title('Training...')
+    plt.xlabel('Episode')
+    plt.ylabel('Duration')
+    plt.plot(durations_t.numpy())
+    # Take 100 episode averages and plot them too
+    if len(durations_t) >= 100:
+        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
+        means = torch.cat((torch.zeros(99), means))
+        plt.plot(means.numpy())
+
+    plt.pause(0.001)  # pause a bit so that plots are updated
+    if is_ipython:
+        display.clear_output(wait=True)
+        display.display(plt.gcf())
+
+
+def optimize_model():
+    if len(memory) < BATCH_SIZE:
+        return
+    transitions = memory.sample(BATCH_SIZE)
+
+    batch = Transition(*zip(*transitions))
+
+    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
+                                            batch.next_state)), device=device, dtype=torch.uint8)
+    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
+
+    state_batch = torch.cat(batch.state)
+    action_batch = torch.cat(batch.action)
+    reward_batch = torch.cat(batch.reward)
+
+    state_action_values = policy_net(state_batch).gather(1, action_batch)
+
+    next_state_values = torch.zeros(BATCH_SIZE, device=device)
+    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()
+    expected_state_action_values = (next_state_values * GAMMA) + reward_batch
+    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))
+
+    optimizer.zero_grad()
+    loss.backward()
+
+    for param in policy_net.parameters():
+        param.grad.data.clamp_(-1, 1)
+    optimizer.step()
+
+num_episodes = 300
+for i_episode in range(num_episodes):
+    env.reset()
+    last_screen = get_screen()
+    current_screen = get_screen()
+    state = current_screen - last_screen
+    for t in count():
+        action = select_action(state)
+        _, reward, done, _ = env.step(action)
+        reward = torch.tensor([reward], device=device)
+
+        last_screen = current_screen
+        current_screen = get_screen()
+        if not done:
+            next_state = current_screen - last_screen
+        else:
+            next_state = None
+
+        memory.push(state, action, next_state, reward)
+
+        state = next_state
+
+        optimize_model()
+        if done:
+            episode_durations.append(t + 1)
+            plot_durations()
+            break
+    if i_episode % TARGET_UPDATE == 0:
+        target_net.load_state_dict(policy_net.state_dict())
+print('Complete')
+env.render()
+env.close()
+env.ioff()
+env.show()
+
diff --git a/Test/test3.py b/Test/test3.py
index b8fec91..731fc1d 100644
--- a/Test/test3.py
+++ b/Test/test3.py
@@ -1,5 +1,6 @@
 import gym
 import numpy as np
+import random
 
 env = gym.make('CartPole-v0')
 env.reset()
@@ -14,9 +15,3 @@ print(env.observation_space.sample())
 print(env.action_space.n)
 print(env.reset())
 env.close()
-
-a = np.zeros(shape=(10, ), dtype=np.float32)
-print(a)
-
-li = [lambda x:x for x in range(10)]
-print(li[0](91))
diff --git a/args.py b/args.py
index af6e0e9..f008c9f 100644
--- a/args.py
+++ b/args.py
@@ -30,7 +30,7 @@ parser.add_argument('--end-epsilon', type=float, default=0.1)
 parser.add_argument('--noisy-net-sigma', type=float, default=None)
 parser.add_argument('--demo', action='store_true', default=False)
 parser.add_argument('--load', type=str, default=None)
-parser.add_argument('--steps', type=int, default=10 ** 5)
+parser.add_argument('--steps', type=int, default=10 ** 2)
 parser.add_argument('--prioritized-replay', action='store_true')
 parser.add_argument('--replay-start-size', type=int, default=1000)
 parser.add_argument('--target-update-interval', type=int, default=10 ** 2)
